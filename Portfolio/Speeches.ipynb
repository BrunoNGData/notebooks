{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow on Textual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook contains the code to predict the authors of texts using a convolutional neural network. Three different data representations are used.\n",
    "* First, the texts are treated as simple sequences of their words.\n",
    "* Second, the texts are splitted into its sentences. Then, the author probabilities on these sentences vote on the text author.\n",
    "* Third, the texts are represented by a three-dimensional tensor (with an axis for sentence, word and embedding), for 3D convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These are the packages used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 1. Text Preproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import\n",
    "Load data from sqlite file into pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>president</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>January 9, 2016</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Hi, everybody. Seven years ago, the American a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>January 30, 2016</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Hi, everybody. As I said in my State of the Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>February 6, 2016</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Hi, everybody. One of the things that makes Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>February 27, 2016</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Hi, everybody. This week, we continued our mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>January 23, 2016</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Hi, everybody. When I took office 7 years ago ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date     president  \\\n",
       "0    January 9, 2016  Barack Obama   \n",
       "1   January 30, 2016  Barack Obama   \n",
       "2   February 6, 2016  Barack Obama   \n",
       "3  February 27, 2016  Barack Obama   \n",
       "4   January 23, 2016  Barack Obama   \n",
       "\n",
       "                                                text  \n",
       "0  Hi, everybody. Seven years ago, the American a...  \n",
       "1  Hi, everybody. As I said in my State of the Un...  \n",
       "2  Hi, everybody. One of the things that makes Am...  \n",
       "3  Hi, everybody. This week, we continued our mis...  \n",
       "4  Hi, everybody. When I took office 7 years ago ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('/Users/desiredewaele/Google Drive/Datasets/rawData.sqlite')\n",
    "speeches = pd.read_sql(\"select date, president, text from Speeches where speech is 'Weekly Address'\", conn)\n",
    "speeches = speeches[speeches.president.str.contains('Barack Obama|William J. Clinton|George W. Bush', regex=True)]\n",
    "speeches.reset_index(inplace=True, drop=True)\n",
    "speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Date Formatting\n",
    "Get the dates in a sortable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getDates(date):\n",
    "    date = dt.datetime.strptime(date, '%B %d, %Y')\n",
    "    date = dt.datetime.strftime(date, '%Y-%m-%d')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>president</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993-02-06</td>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>Good morning. This is Bill Clinton. And this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993-02-13</td>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>Good afternoon, my fellow Americans. On Wednes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-02-20</td>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>This is Bill Clinton. As you know, this week I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993-02-27</td>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>Good morning. Before I talk with you about our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993-03-06</td>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>Good morning. We've come a long way together i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           president  \\\n",
       "0  1993-02-06  William J. Clinton   \n",
       "1  1993-02-13  William J. Clinton   \n",
       "2  1993-02-20  William J. Clinton   \n",
       "3  1993-02-27  William J. Clinton   \n",
       "4  1993-03-06  William J. Clinton   \n",
       "\n",
       "                                                text  \n",
       "0  Good morning. This is Bill Clinton. And this m...  \n",
       "1  Good afternoon, my fellow Americans. On Wednes...  \n",
       "2  This is Bill Clinton. As you know, this week I...  \n",
       "3  Good morning. Before I talk with you about our...  \n",
       "4  Good morning. We've come a long way together i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches.date = speeches.date.apply(getDates)\n",
    "speeches.sort_values('date', inplace=True)\n",
    "speeches.reset_index(drop=True, inplace=True)\n",
    "speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### A. Text Data\n",
    "First, the texts are treated as simple sequences of their words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getFormatText(s):\n",
    "    s = s.lower()                         # Change text to lowercase\n",
    "    s = re.sub('\\(\\.*\\)|\\[\\.*\\]', '', s)  # Removes all bracket parts\n",
    "    s = re.sub('[^(a-z)]| ', ' ', s)      # Change all not-text to spaces\n",
    "    s = re.sub(' +',' ', s)               # Remove all redundant spaces\n",
    "    s = s.strip()                         # Remove outer whitespace\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>good morning this is bill clinton and this mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>good afternoon my fellow americans on wednesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>this is bill clinton as you know this week i s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>good morning before i talk with you about our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>good morning we ve come a long way together in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            president                                               text\n",
       "0  William J. Clinton  good morning this is bill clinton and this mor...\n",
       "1  William J. Clinton  good afternoon my fellow americans on wednesda...\n",
       "2  William J. Clinton  this is bill clinton as you know this week i s...\n",
       "3  William J. Clinton  good morning before i talk with you about our ...\n",
       "4  William J. Clinton  good morning we ve come a long way together in..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.DataFrame({'text': speeches.text.apply(getFormatText), 'president': speeches.president})\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### B. Sentence Data\n",
    "Second, the texts are splitted into its sentences. Later, the author probabilities on the sentences of a given text will vote on the author of that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getSentences(text):\n",
    "    sentences = np.array(re.split('\\. |\\! |\\? |\\; |\\: |\\\" ', text))\n",
    "    for i, s in enumerate(sentences):\n",
    "        sentences[i] = getFormatText(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>this is bill clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>and this morning on my first radio address i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>how we can build a strong and growing economy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William J. Clinton</td>\n",
       "      <td>lately we ve had some good news about our economy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              president                                           sentence\n",
       "0 0  William J. Clinton                                       good morning\n",
       "  1  William J. Clinton                               this is bill clinton\n",
       "  2  William J. Clinton  and this morning on my first radio address i w...\n",
       "  3  William J. Clinton  how we can build a strong and growing economy ...\n",
       "  4  William J. Clinton  lately we ve had some good news about our economy"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = speeches.text.apply(getSentences).apply(pd.Series, 1).stack()\n",
    "presidents = speeches.president[sentence.index.get_level_values(0)].values\n",
    "sentences = pd.DataFrame({'sentence': sentence, 'president': presidents})\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### C. Tensor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Third, the texts are represented by a three-dimensional tensor for 3D convolution.\n",
    "* Axis 1 will represent the sequence of sentences within a text.\n",
    "* Axis 2 will represent the sequence of words within a sentence.\n",
    "* Axis 3 will represent the embedding representation of a particular word (see later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[good, morning]</td>\n",
       "      <td>[this, is, bill, clinton]</td>\n",
       "      <td>[and, this, morning, on, my, first, radio, add...</td>\n",
       "      <td>[how, we, can, build, a, strong, and, growing,...</td>\n",
       "      <td>[lately, we, ve, had, some, good, news, about,...</td>\n",
       "      <td>[our, business, productivity, is, up]</td>\n",
       "      <td>[our, people, are, producing, more, at, lower,...</td>\n",
       "      <td>[and, lower, interest, rates, are, giving, peo...</td>\n",
       "      <td>[now, that, change, is, in, the, air, people, ...</td>\n",
       "      <td>[consumer, confidence, is, up, and, the, finan...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[good, afternoon, my, fellow, americans]</td>\n",
       "      <td>[on, wednesday, night, i, will, present, my, p...</td>\n",
       "      <td>[this, morning, i, want, to, talk, with, you, ...</td>\n",
       "      <td>[as, i, have, traveled, our, country, over, th...</td>\n",
       "      <td>[that, theme, is, the, need, for, change]</td>\n",
       "      <td>[bold, comprehensive, change, to, reverse, the...</td>\n",
       "      <td>[over, the, last, years, while, the, middle, c...</td>\n",
       "      <td>[higher, deficits, came, with, lower, taxes, o...</td>\n",
       "      <td>[and, those, deficits, forced, government, to,...</td>\n",
       "      <td>[good, families, in, embattled, neighborhoods,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0    \\\n",
       "0                           [good, morning]   \n",
       "1  [good, afternoon, my, fellow, americans]   \n",
       "\n",
       "                                                 1    \\\n",
       "0                          [this, is, bill, clinton]   \n",
       "1  [on, wednesday, night, i, will, present, my, p...   \n",
       "\n",
       "                                                 2    \\\n",
       "0  [and, this, morning, on, my, first, radio, add...   \n",
       "1  [this, morning, i, want, to, talk, with, you, ...   \n",
       "\n",
       "                                                 3    \\\n",
       "0  [how, we, can, build, a, strong, and, growing,...   \n",
       "1  [as, i, have, traveled, our, country, over, th...   \n",
       "\n",
       "                                                 4    \\\n",
       "0  [lately, we, ve, had, some, good, news, about,...   \n",
       "1          [that, theme, is, the, need, for, change]   \n",
       "\n",
       "                                                 5    \\\n",
       "0              [our, business, productivity, is, up]   \n",
       "1  [bold, comprehensive, change, to, reverse, the...   \n",
       "\n",
       "                                                 6    \\\n",
       "0  [our, people, are, producing, more, at, lower,...   \n",
       "1  [over, the, last, years, while, the, middle, c...   \n",
       "\n",
       "                                                 7    \\\n",
       "0  [and, lower, interest, rates, are, giving, peo...   \n",
       "1  [higher, deficits, came, with, lower, taxes, o...   \n",
       "\n",
       "                                                 8    \\\n",
       "0  [now, that, change, is, in, the, air, people, ...   \n",
       "1  [and, those, deficits, forced, government, to,...   \n",
       "\n",
       "                                                 9   ... 144 145 146 147 148  \\\n",
       "0  [consumer, confidence, is, up, and, the, finan... ...  []  []  []  []  []   \n",
       "1  [good, families, in, embattled, neighborhoods,... ...  []  []  []  []  []   \n",
       "\n",
       "  149 150 151 152 153  \n",
       "0  []  []  []  []  []  \n",
       "1  []  []  []  []  []  \n",
       "\n",
       "[2 rows x 154 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = speeches.text.apply(getSentences).apply(pd.Series)\n",
    "tensor = tensor.fillna('').applymap(lambda x: x.split())\n",
    "tensor.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 2. Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Given a collection of texts with n different words, one can imagine an n-dimensional vector space in which every word could be represented by one-hot encoding - i.e. a value of 1 for the given word dimension, and a value of 0 for all other dimensions.\n",
    "\n",
    "Embeddings are representations of words in a reduced vector space, not with n dimensions, but with, say 100. They are trained in such a way that words with related meanings have smaller distances in between them, whereas unrelated words will lie widely apart in the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = sentences.sentence.str.split().values\n",
    "words = np.array([item for sublist in words for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function builds the dataset. \n",
    "* Counts stores the 15000 most occuring words with their number of occurences in a tuple.\n",
    "* Dictionary stores the same words with their index in count.\n",
    "* Reverse Dictionary swaps the key value pairs in dictionary.\n",
    "* Data gets the index in dictionary for all words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildDataset(words, VOCAB=15000):\n",
    "    \n",
    "    # Count [['UNK', 52101], ('the', 43154), ('to', 33493) ...]\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(VOCAB - 1))\n",
    "    \n",
    "    # Dictionary {'UNK': 0, 'the': 1, 'to': 2, ...}\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    # Reverse Dictionary {0: 'UNK', 1: 'the', 2: 'to', ...}\n",
    "    rDictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "\n",
    "    # Data [61, 124, 13, 14, ...] Index in dictionary\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    \n",
    "    return data, count, dictionary, rDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data, count, dictionary, rDictionary = buildDataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Five most common words:', [['UNK', 595], (u'the', 43182), (u'to', 33498), (u'and', 32856), (u'of', 22917)])\n",
      "('Five first words in texts:', array([u'good', u'morning', u'this', u'is', u'bill', u'clinton', u'and',\n",
      "       u'this'], \n",
      "      dtype='<U25'))\n",
      "('Five first words count index:', [61, 124, 13, 14, 103, 2632, 3, 13])\n"
     ]
    }
   ],
   "source": [
    "print('Five most common words:', count[:5])\n",
    "print('Five first words in texts:', words[:8])\n",
    "print('Five first words count index:', data[:8])\n",
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Generate Training Batches\n",
    "The idea is to create word pairs of data and label words. Every word of the data set (or batch) is put together with a close occuring word in a tuple. Windows is the number of words to consider as a label left and right of the query word. Skips is the number of times we reuse a word to generate a label, so this should be two times the window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"img/skipgramdata.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generateBatch(BATCH, SKIPS, WINDOW):\n",
    "    global data_index\n",
    "    assert BATCH % SKIPS == 0\n",
    "    assert SKIPS <= 2 * WINDOW\n",
    "    batch = np.ndarray(shape=(BATCH), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(BATCH, 1), dtype=np.int32)\n",
    "    span = 2 * WINDOW + 1\n",
    "    buffer = collections.deque(maxlen=span) # Create enhanced array\n",
    "    for _ in range(span): # Add count index of every data word in span\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(BATCH // SKIPS):\n",
    "        target = WINDOW  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ WINDOW ]\n",
    "        for j in range(SKIPS):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * SKIPS + j] = buffer[WINDOW]\n",
    "            labels[i * SKIPS + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', [u'good', u'morning', u'this', u'is', u'bill', u'clinton', u'and', u'this'])\n",
      "\n",
      "With SKIPS = 4 and WINDOW = 2:\n",
      "('    batch:', [u'this', u'this', u'this', u'this', u'is', u'is', u'is', u'is'])\n",
      "('    labels:', [u'good', u'is', u'morning', u'bill', u'clinton', u'morning', u'this', u'bill'])\n",
      "\n",
      "With SKIPS = 2 and WINDOW = 1:\n",
      "('    batch:', [u'morning', u'morning', u'this', u'this', u'is', u'is', u'bill', u'bill'])\n",
      "('    labels:', [u'good', u'this', u'morning', u'is', u'this', u'bill', u'is', u'clinton'])\n"
     ]
    }
   ],
   "source": [
    "print('data:', [rDictionary[di] for di in data[:8]])\n",
    "\n",
    "for SKIPS, WINDOW in [(4, 2), (2, 1)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generateBatch(8, SKIPS, WINDOW)\n",
    "    print('\\nWith SKIPS = %d and WINDOW = %d:' % (SKIPS, WINDOW))\n",
    "    print('    batch:', [rDictionary[bi] for bi in batch])\n",
    "    print('    labels:', [rDictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Train Embeddings with Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters for skip-gram model. We pick a random validation set to sample nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH = 200\n",
    "EMBED = 128\n",
    "WINDOW = 10\n",
    "STEPS = 200001\n",
    "VOCAB = 15000\n",
    "RATE = 1.0\n",
    "NEGSAMPLES = 20 # Number of negative examples to sample.\n",
    "VALIDEXAMPLES = np.array([dictionary[x] for x in ['france', 'mother', 'week', 'america', 'washington']])\n",
    "#np.array(random.sample(population=range(100), k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tfTrainX = tf.placeholder(shape=[BATCH], dtype=tf.int32)\n",
    "    tfTrainY = tf.placeholder(shape=[BATCH, 1], dtype=tf.int32)\n",
    "    tfValidX = tf.constant(VALIDEXAMPLES, dtype=tf.int32)\n",
    "\n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(tf.random_uniform([VOCAB, EMBED], -1.0, 1.0)) \n",
    "    weights = tf.Variable(tf.truncated_normal([VOCAB, EMBED], stddev=1.0 / np.sqrt(EMBED)))\n",
    "    biases = tf.Variable(tf.zeros([VOCAB]))\n",
    "\n",
    "    # Look up embeddings and compute the softmax loss, using a sample of the negative labels each time\n",
    "    embed = tf.nn.embedding_lookup(embeddings, tfTrainX)\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=weights, biases=biases, \n",
    "        inputs=embed, labels=tfTrainY, num_sampled=NEGSAMPLES, num_classes=VOCAB))\n",
    "\n",
    "    # Optimizing embeddings, weights and biases\n",
    "    optimizer = tf.train.AdagradOptimizer(RATE).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    normEmbeddings = embeddings / tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    validEmbeddings = tf.nn.embedding_lookup(normEmbeddings, tfValidX)\n",
    "    similarity = tf.matmul(validEmbeddings, tf.transpose(normEmbeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss at step 0: 6.25889539719\n",
      "Nearest to france: diagnosed, searing, conservationists, began, drowned, moderation, zarqawi, motives,\n",
      "Nearest to mother: evil, charter, kevin, sectors, projection, earful, unbearable, tag,\n",
      "Nearest to week: handmade, tendency, says, wake, permanently, weakens, early, outstanding,\n",
      "Nearest to america: relaxing, accomplishes, sneak, ame, aggregate, arsenic, stocks, retired,\n",
      "Nearest to washington: system, settlers, safest, louis, leaner, leaking, enlarge, squarely,\n",
      "\n",
      "Average loss at step 50000: 3.01425728183\n",
      "Nearest to france: searing, diagnosed, little, conservationists, drowned, monoxide, between, moderation,\n",
      "Nearest to mother: father, kevin, unbearable, charter, projection, exporting, evil, responds,\n",
      "Nearest to week: good, morning, listening, you, house, thank, i, this,\n",
      "Nearest to america: our, world, to, of, and, the, for, nation,\n",
      "Nearest to washington: hernandez, congress, politics, outcompeted, practicing, benches, unrelated, prince,\n",
      "\n",
      "Average loss at step 100000: 2.88158141371\n",
      "Nearest to france: diagnosed, searing, monoxide, between, conservationists, drowned, locally, little,\n",
      "Nearest to mother: father, letter, you, openings, evil, later, day, parent,\n",
      "Nearest to week: morning, good, monday, for, i, year, tuesday, this,\n",
      "Nearest to america: and, the, of, all, we, our, with, to,\n",
      "Nearest to washington: politics, chemistry, emitters, prince, robert, swarmed, youngsters, practicing,\n",
      "\n",
      "Average loss at step 150000: 2.83962658428\n",
      "Nearest to france: diagnosed, fema, conservationists, monoxide, drowned, searing, mailed, lightning,\n",
      "Nearest to mother: day, father, weekend, letter, you, parent, honor, let,\n",
      "Nearest to week: listening, morning, weekend, great, earlier, this, ll, thank,\n",
      "Nearest to america: world, the, growing, that, economy, states, cutters, is,\n",
      "Nearest to washington: politics, political, governance, siblings, heard, some, youngsters, medications,\n",
      "\n",
      "Average loss at step 200000: 2.81931251383\n",
      "Nearest to france: fema, diagnosed, countries, conservationists, lightning, nelson, moderation, drowned,\n",
      "Nearest to mother: father, her, letter, daughter, husband, openings, mom, roadblock,\n",
      "Nearest to week: earlier, morning, listening, days, good, trip, my, thank,\n",
      "Nearest to america: nation, world, century, so, our, st, strong, barbed,\n",
      "Nearest to washington: politics, fiscal, debate, youngsters, boils, presented, important, fifty,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    averageLoss = 0\n",
    "    for step in range(STEPS):\n",
    "        batchX, batchY = generateBatch(BATCH, WINDOW*2, WINDOW)\n",
    "        _, l = session.run([optimizer, loss], {tfTrainX: batchX, tfTrainY: batchY})\n",
    "        averageLoss += l\n",
    "        if step % 50000 == 0:\n",
    "            averageLoss /= 50000 if step > 0 else 1\n",
    "            print('\\nAverage loss at step {}: {}'.format(step, averageLoss))\n",
    "            averageLoss = 0\n",
    "            \n",
    "            sim = similarity.eval()\n",
    "            for i in range(len(VALIDEXAMPLES)):\n",
    "                nearest = (-sim[i, :]).argsort()[1:8+1] # 8 nearest neighbors\n",
    "                log = 'Nearest to {}:'.format(rDictionary[VALIDEXAMPLES[i]])\n",
    "                for k in range(len(nearest)):\n",
    "                    log = '{} {},'.format(log, rDictionary[nearest[k]])\n",
    "                print(log)\n",
    "    finalEmbeddings = normEmbeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plotWithLabels(low_dim_embs, labels, filename='img/tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "low_dim_embs = tsne.fit_transform(finalEmbeddings[:500, :]) # Plot only 500\n",
    "labels = [rDictionary[i] for i in xrange(500)]\n",
    "plotWithLabels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.save('/Users/desiredewaele/Google Drive/Datasets/embeddings.npy', finalEmbeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 3. Classify Texts with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Gather Embed IDs en padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "finalEmbeddings = np.load('/Users/desiredewaele/Google Drive/Datasets/embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "textLength = max(texts.text.apply(lambda x: len(x.split())))\n",
    "sentenceLength = max(sentences.sentence.apply(lambda x: len(x.split())))\n",
    "\n",
    "print 'The longest text contains {} words.'.format(textLength)\n",
    "print 'The longest sentence contains {} words.'.format(sentenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getEmbed(words):\n",
    "    ids = np.empty(words.size, int)\n",
    "    for i, word in enumerate(words):\n",
    "        ids[i] = dictionary[word] if word in dictionary else 0\n",
    "    return ids\n",
    "    \n",
    "def pad(text, textlength):\n",
    "    words = np.array(text.split())\n",
    "    ids = getEmbed(words)\n",
    "    pad = np.zeros(textlength - len(words), int)\n",
    "    return np.append(ids, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataT = pd.DataFrame(texts.text.apply(lambda x: pad(x, textLength)).tolist(), index=texts.index)\n",
    "dataS = pd.DataFrame(sentences.sentence.apply(lambda x: pad(x, sentenceLength)).tolist(), index=sentences.index)\n",
    "print dataT.shape\n",
    "print dataS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataT.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labelsT = texts.president\n",
    "labelsT.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataS.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labelsS = sentences.president\n",
    "labelsS.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Label One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training sentences', dataS.shape, labelsS.shape)\n",
    "print('Training texts', dataT.shape, labelsT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labelsS = pd.get_dummies(labelsS)\n",
    "labelsT = pd.get_dummies(labelsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training sentences', dataS.shape, labelsS.shape)\n",
    "print('Training texts', dataT.shape, labelsT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training, validation and testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sentence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "indeces = range(len(speeches))\n",
    "np.random.shuffle(indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainX = dataS.loc[indeces[:800]].sample(frac=1, random_state=1)\n",
    "validX = dataS.loc[indeces[800:1000]].sample(frac=1, random_state=1)\n",
    "testX = dataS.loc[indeces[1000:]].sample(frac=1, random_state=1)\n",
    "\n",
    "trainY = labelsS.loc[indeces[:800]].sample(frac=1, random_state=1)\n",
    "validY = labelsS.loc[indeces[800:1000]].sample(frac=1, random_state=1)\n",
    "testY = labelsS.loc[indeces[1000:]].sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Textual Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#trainValidX, testX, trainValidY, testY = train_test_split(dataT, labelsT, test_size=0.2, random_state=100)\n",
    "#trainX, validX, trainY, validY = train_test_split(trainValidX, trainValidY, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training set:', trainX.shape, trainY.shape)\n",
    "print('Validation set:', validX.shape, validY.shape)\n",
    "print('Testing set:', testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Conv Net to Classify Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"img/TextCNN.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH = 300 #20\n",
    "TEXTLENGTH = sentenceLength #textLength\n",
    "LABELS = 3\n",
    "CHANNELS = 1\n",
    "FILTERSIZES = [4, 5, 6]\n",
    "FILTERS = 128\n",
    "RATE = 1e-2\n",
    "DROPOUT = 0.6\n",
    "L2 = 1\n",
    "STEPS = 2001 #251\n",
    "BREAKS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 'All Embedings ({}, {})'.format(VOCAB, EMBED)\n",
    "print 'Batch Embedings ({}, {}, {}, {})'.format(BATCH, TEXTLENGTH, EMBED, CHANNELS)\n",
    "print 'Filters ({}, {}, {}, {})'.format(FILTERSIZES, EMBED, CHANNELS, FILTERS)\n",
    "print 'Conv Scores ({}, {}, {}, {})'.format(BATCH, TEXTLENGTH - FILTERSIZES + 1, 1, FILTERS)\n",
    "print 'Max Pools ({}, {})'.format(BATCH, len(FILTERSIZES) * FILTERS)\n",
    "print 'Logits ({}, {})'.format(BATCH, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Placeholders for input, output and dropout\n",
    "    tfDataX = tf.placeholder(tf.int32, shape=[None, TEXTLENGTH])\n",
    "    tfDataY = tf.placeholder(tf.float32, shape=[None, LABELS])\n",
    "    tfDrop = tf.placeholder(tf.float32)\n",
    "            \n",
    "    # Gather relevant embeddings\n",
    "    # w = tf.Variable(tf.random_uniform([VOCAB, EMBED], -1.0, 1.0))\n",
    "    embeddings = tf.nn.embedding_lookup(finalEmbeddings, tfDataX)\n",
    "    embeddings = tf.expand_dims(embeddings, -1)\n",
    "    l2Loss = tf.constant(0.0)\n",
    "\n",
    "    pooled_outputs = []\n",
    "    for filterSize in FILTERSIZES:\n",
    "        \n",
    "        # Convolutional Variables\n",
    "        weights = tf.Variable(tf.truncated_normal([filterSize, EMBED, 1, FILTERS], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[FILTERS]))\n",
    "        \n",
    "        # Convolution Layer\n",
    "        x = tf.nn.conv2d(embeddings, weights, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        x = tf.nn.relu(tf.nn.bias_add(x, bias))\n",
    "\n",
    "        # Max-pooling over the outputs\n",
    "        pooled = tf.nn.max_pool(x, ksize=[1, TEXTLENGTH - filterSize + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "    # Combine all pooled features\n",
    "    totalFilters = FILTERS * len(FILTERSIZES)\n",
    "    x = tf.concat(pooled_outputs, axis=3)\n",
    "    x = tf.reshape(x, [-1, totalFilters])\n",
    "\n",
    "    # Add dropout\n",
    "    x = tf.nn.dropout(x, tfDrop)\n",
    "\n",
    "    # Affine Layer\n",
    "    weights = tf.Variable(tf.truncated_normal([totalFilters, LABELS], stddev=0.1))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[LABELS]))\n",
    "    l2Loss += tf.nn.l2_loss(weights) + tf.nn.l2_loss(bias)\n",
    "    logits = tf.nn.xw_plus_b(x, weights, bias)\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    globalStep = tf.Variable(initial_value=0, trainable=False)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tfDataY)) + L2*l2Loss\n",
    "    optimizer = tf.train.AdamOptimizer(RATE)\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    training = optimizer.apply_gradients(gradients, global_step=globalStep)\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    predictions = {\"classes\": tf.argmax(logits, 1), \"probabilities\": tf.nn.softmax(logits)}\n",
    "    correctPredictions = tf.equal(predictions['classes'], tf.argmax(tfDataY, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPredictions, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"{:>8} {:>16} {:>20} {:>20}\".format('Time', 'Step', 'Loss Train Valid', 'Acc Train Valid'))  \n",
    "    history = []\n",
    "    for step in range(STEPS):\n",
    "        offset = (step * BATCH) % (len(trainX) - BATCH)\n",
    "        batchX = trainX.values[offset:(offset + BATCH), :]\n",
    "        batchY = trainY.values[offset:(offset + BATCH), :]\n",
    "        trainFeed = {tfDataX: batchX, tfDataY: batchY, tfDrop: DROPOUT}\n",
    "        _, globalstep, trainLoss, trainAcc = session.run([training, globalStep, loss, accuracy], trainFeed)\n",
    "        if(step % (STEPS // BREAKS) == 0):\n",
    "            validFeed = {tfDataX: validX, tfDataY: validY, tfDrop: 1}\n",
    "            validLoss, validAcc = session.run([loss, accuracy], validFeed)\n",
    "            history.append((trainAcc, validAcc, trainLoss, validLoss))\n",
    "            print(\"{:%H:%M:%S} {g:>8} of {s:>4} {tl:>14.2f}, {vl:>1.2f} {ta:>14.2f}, {va:>1.2f}\".format(\n",
    "                dt.datetime.now(), g=globalstep, s=STEPS, tl=trainLoss, vl=validLoss, ta=trainAcc, va=validAcc))\n",
    "    testFeed = {tfDataX: testX, tfDataY: testY, tfDrop: 1}\n",
    "    testLoss, testAcc, testPred = session.run([loss, accuracy, predictions], testFeed)\n",
    "    print(\"\\nTest Loss & Acc: {tl:>1.2f}, {ta:>1.2f}\".format(tl=testLoss, ta=testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "at, av, lt, lv = zip(*history)\n",
    "fig = plt.figure(figsize=(15, 8)); ax1 = fig.add_subplot(221); ax2 = fig.add_subplot(222)\n",
    "\n",
    "ax1.plot(np.arange(0, len(at), 1), at,\".-\", color='#2A6EA6', label=\"Training: {0:.2f}%\".format(at[-1]))\n",
    "ax1.plot(np.arange(0, len(av), 1), av,\".-\", color='#FFA933', label=\"Validation: {0:.2f}%\".format(av[-1]))\n",
    "ax1.grid(True); ax1.legend(loc=\"lower right\"); ax1.set_title(\"Accuracy per epoch\")\n",
    "\n",
    "ax2.plot(np.arange(0, len(lt), 1), lt,\".-\", color='#2A6EA6', label=\"Training: {0:.2f}\".format(lt[-1]))\n",
    "ax2.plot(np.arange(0, len(lv), 1), lv,\".-\", color='#FFA933', label=\"Validation: {0:.2f}\".format(lv[-1]))\n",
    "ax2.grid(True); ax2.legend(loc=\"upper right\"); ax2.set_title(\"Cost per epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Sentence Voting to Classify Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Probabilities per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentenceProbs = pd.DataFrame(testPred['probabilities'], index=testY.index).sort_index()\n",
    "sentenceProbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Probabilities per text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "textProbs = sentenceProbs.groupby(level=0).mean()\n",
    "textProbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compare classes and labels per text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "textClasses = np.argmax(textProbs.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentenceLabels = pd.Series(np.argmax(testY.values, 1), index=testY.index)\n",
    "textLabels = sentenceLabels.groupby(level=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.mean(textClasses == textLabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
