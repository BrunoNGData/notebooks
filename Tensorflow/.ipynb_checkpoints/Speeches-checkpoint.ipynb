{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow on Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>president</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>2016-12-17</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>hi everybody if you ve ever played a game of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>2016-12-24</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>the president merry christmas everybody one of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>happy new year everybody at a time when we tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>since the days of george washington presidents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>2017-01-14</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>this week i traveled to chicago to deliver my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date     president  \\\n",
       "1575  2016-12-17  Barack Obama   \n",
       "1573  2016-12-24  Barack Obama   \n",
       "1576  2016-12-31  Barack Obama   \n",
       "1579  2017-01-07  Barack Obama   \n",
       "1581  2017-01-14  Barack Obama   \n",
       "\n",
       "                                                   text  \n",
       "1575  hi everybody if you ve ever played a game of b...  \n",
       "1573  the president merry christmas everybody one of...  \n",
       "1576  happy new year everybody at a time when we tur...  \n",
       "1579  since the days of george washington presidents...  \n",
       "1581  this week i traveled to chicago to deliver my ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('/Users/desiredewaele/Google Drive/Datasets/tidyData.sqlite')\n",
    "speeches = pd.read_sql(\"select date, president, text from Speeches where speech is 'Weekly Address'\", conn)\n",
    "speeches = speeches[speeches.president.str.contains('Barack Obama|William J. Clinton|George W. Bush', regex=True)]\n",
    "speeches = speeches.sort_values('date', ascending=True)\n",
    "speeches.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = sum(list(speeches.text.apply(lambda x: x.split())), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Total number of words:', len(words))\n",
    "print('Number of unique words:', len(set(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Data Pre-Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildDataset(words, VOCAB):\n",
    "    # Count is a list of tuples with frequent words and # occurences\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(VOCAB - 1))\n",
    "    # Dictionary is a dictionary with all frequent words and count index\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    # Data is a list containing count index for all words\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    rDictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, rDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "VOCAB = 1000\n",
    "data, count, dictionary, rDictionary = buildDataset(words, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Five most common words:', count[:5])\n",
    "print('Five first words in speeches:', words[:8])\n",
    "print('Five first words count index:', data[:8])\n",
    "#del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Generate Training Batches\n",
    "This function generates two arrays. Batch contains a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generateBatch(BATCH, SKIPS, WINDOW):\n",
    "    global data_index\n",
    "    assert BATCH % SKIPS == 0\n",
    "    assert SKIPS <= 2 * WINDOW\n",
    "    batch = np.ndarray(shape=(BATCH), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(BATCH, 1), dtype=np.int32)\n",
    "    span = 2 * WINDOW + 1 # Consider WINDOW words left & right\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(BATCH // SKIPS):\n",
    "        target = WINDOW  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ WINDOW ]\n",
    "        for j in range(SKIPS):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * SKIPS + j] = buffer[WINDOW]\n",
    "            labels[i * SKIPS + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('data:', [rDictionary[di] for di in data[:8]])\n",
    "\n",
    "for SKIPS, WINDOW in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generateBatch(16, SKIPS, WINDOW)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (SKIPS, WINDOW))\n",
    "    print('    batch:', [rDictionary[bi] for bi in batch])\n",
    "    print('    labels:', [rDictionary[li] for li in labels.reshape(16)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Train Embeddings with Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH = 128\n",
    "EMBED = 8 # Dimension of the embedding vector.\n",
    "WINDOW = 1 # How many words to consider left and right.\n",
    "SKIPS = 2 # How many times to reuse an input to generate a label.\n",
    "STEPS = 100001\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the validation samples \n",
    "# to the words that have a low numeric ID, which by construction are also the most frequent. \n",
    "valid_size = 5 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    tfTrainX = tf.placeholder(tf.int32, shape=[BATCH])\n",
    "    tfTrainY = tf.placeholder(tf.int32, shape=[BATCH, 1])\n",
    "    tfValidX = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(tf.random_uniform([VOCAB, EMBED], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([VOCAB, EMBED], stddev=1.0 / math.sqrt(EMBED)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([VOCAB]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, tfTrainX)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_weights, biases=softmax_biases, inputs=embed, \n",
    "        labels=tfTrainY, num_sampled=num_sampled, num_classes=VOCAB))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, tfValidX)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    average_loss = 0\n",
    "    for step in range(STEPS):\n",
    "        batchX, batchY = generateBatch(BATCH, SKIPS, WINDOW)\n",
    "        _, l = session.run([optimizer, loss], {tfTrainX : batchX, tfTrainY : batchY})\n",
    "        average_loss += l\n",
    "        if step % 50000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 50000\n",
    "            # The average loss is an estimate of the loss over the last 50000 batches.\n",
    "            print('\\nAverage loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "            \n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = rDictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = rDictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:500, :]) # Plot only 500\n",
    "labels = [rDictionary[i] for i in xrange(500)]\n",
    "plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longest = max(speeches.text.apply(lambda x: len(x)))\n",
    "def pad(speech):\n",
    "    speech = np.asarray(speech.split())\n",
    "    pad = np.chararray((longest - len(speech),))\n",
    "    pad[:] = '|'\n",
    "    return np.append(speech, pad)\n",
    "\n",
    "words = speeches.text.apply(pad).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1227"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1227 into shape (13946)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-93eee826c176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1227 into shape (13946)"
     ]
    }
   ],
   "source": [
    "words.reshape((-1, longest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1227,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentenceToTensors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d95d977a8abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceToTensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentenceToTensors' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = np.apply_along_axis(sentenceToTensors, 0, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentenceToTensors(speech):\n",
    "    embedList = np.empty(len(speech))\n",
    "    for i, w in enumerate(speech):\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        else:\n",
    "            index = 0\n",
    "        np.append(embedList, final_embeddings[index])\n",
    "    return embedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentenceToTensors(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tfDataX = tf.placeholder(tf.float32, shape=(None, SIZE, SIZE))\n",
    "    tfDataY = tf.placeholder(tf.float32, shape=(None, LABELS))\n",
    "\n",
    "    # Variables.\n",
    "    w1 = tf.Variable(tf.truncated_normal([PATCH, PATCH, CHANNELS, DEPTH], stddev=0.1))\n",
    "    w2 = tf.Variable(tf.truncated_normal([PATCH, PATCH, DEPTH, 2*DEPTH], stddev=0.1))\n",
    "    w3 = tf.Variable(tf.truncated_normal([SIZE // 4 * SIZE // 4 * 2*DEPTH, HIDDEN], stddev=0.1))\n",
    "    w4 = tf.Variable(tf.truncated_normal([HIDDEN, LABELS], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([DEPTH]))\n",
    "    b2 = tf.Variable(tf.constant(1.0, shape=[2*DEPTH]))\n",
    "    b3 = tf.Variable(tf.constant(1.0, shape=[HIDDEN]))\n",
    "    b4 = tf.Variable(tf.constant(1.0, shape=[LABELS]))\n",
    "\n",
    "    # Model.\n",
    "    def model(x):\n",
    "        # Convolutional layer 1\n",
    "        x = tf.nn.conv2d(x, w1, [1, 1, 1, 1], padding='SAME') + b1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        # Convolutional layer 2\n",
    "        x = tf.nn.conv2d(x, w2, [1, 1, 1, 1], padding='SAME') + b2\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        # Fully connected layer\n",
    "        x = tf.reshape(x, (-1, SIZE // 4 * SIZE // 4 * 2*DEPTH))\n",
    "        x = tf.nn.relu(tf.matmul(x, w3) + b3)\n",
    "        return tf.matmul(x, w4) + b4\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tfDataX)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tfDataY))\n",
    "    optimizer = tf.train.RMSPropOptimizer(RATE).minimize(loss)\n",
    "\n",
    "    # Predictions and Accuracy.\n",
    "    predictions = {\"classes\": tf.argmax(logits, axis=1),\"probabilities\": tf.nn.softmax(logits)}\n",
    "    accuracy = tf.reduce_mean(tf.to_float(tf.equal(predictions[\"classes\"], tf.argmax(tfDataY, axis=1)))) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
